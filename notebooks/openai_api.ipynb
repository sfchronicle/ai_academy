{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with OpenAI's API\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll go over working with OpenAI's API through [their python client](https://github.com/openai/openai-python). Specifically, we'll:\n",
    "\n",
    "- Setup our OpenAI API key.\n",
    "- Explore the API.\n",
    "- Use the API to generate text.\n",
    "- Run the output through OpenAI's moderation endpoint.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "Let's get started by importing our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't already done so, go ahead and create a `.env` file in the same directory as this notebook (notebooks) and fill it out like this:\n",
    "\n",
    "```\n",
    "[openai]\n",
    "OPENAI_API_KEY = sk-*******\n",
    "```\n",
    "\n",
    "You can find your API key [here](https://platform.openai.com/account/api-keys) after your log in. Replace what we have above with your key. Do not use quotes around your key.\n",
    "\n",
    "Once you've done this, we can load our key into this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse config\n",
    "cp = configparser.ConfigParser(interpolation=None)\n",
    "cp.read(\"./.env\")\n",
    "\n",
    "# Let's load the Hearst credentials\n",
    "openai.api_key = cp.get(\"openai\", \"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the API\n",
    "\n",
    "Now that we've loaded our key, let's explore the API.\n",
    "\n",
    "We'll get started by creating some variables that will control how the LLM behaves.\n",
    "\n",
    "First we'll define which model we want to use. We'll use GPT-3.5-turbo. For more sophisticated tasks, you may want to use GPT-4. For a reminder of all of the models available to you and the differences among them, please refer to [OpenAI's documentation](https://platform.openai.com/docs/models/gpt-3-5).\n",
    "\n",
    "Then we'll define the temperature. This setting controls how \"creative\" the model is. A higher temperature will result in more creative output, but it will also be less accurate. A lower temperature will result in more accurate output, but it will also be less creative. The range is 0 to 2. Never go above 1.\n",
    "\n",
    "Finally, we'll define the max_tokens. This tells the LLM the max number of [tokens](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) the model should generate. Remember, OpenAI charges by the token for usage of their API. Inputs are counted toward the cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-3.5-turbo\"\n",
    "TEMPERATURE = 0\n",
    "MAX_TOKENS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our system and user prompts.\n",
    "\n",
    "System prompts tells the llm how to behave. This is something that is defined behind the scenes when you work with chatGPT's app interface. It is not required.\n",
    "\n",
    "User prompts are the prompts that you provide to the llm. These are the prompts that will be used to generate the output. This is what you're used to see with chatGPT's app interface.\n",
    "\n",
    "For this example, we'll have the LLM summarize a news article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"You are a helpful virtual assistant.\"\n",
    "\n",
    "USER_PROMPT = \"\"\"Pretend to be a reporter at the Washington Post. Below I'm providing you a snippet of a news article that is wrapped in triple backticks (```). Summarize the content using easy-to-understand language. Summaries must be one paragraph. Do not make any assumptions. Only pull information from the snippet.\n",
    "\n",
    "```\n",
    "San Antonio will begin offering another nonstop flight in June, this time to Torreón, Mexico — a hub of the electronics and automotive industries.\n",
    "\n",
    "Wednesday's announcement of the new route on Viva Aerobus came a week after the Mexican low-cost carrier said it would begin offering nonstop service in December to Querétaro, Mexico, which also has a strong automotive industry. \n",
    "\n",
    "Both destinations are of potential interest to those involved in the San Antonio region’s growing auto industry and others. \n",
    "\n",
    "Airport Director Jesus Saenz said pursuing a route to Torreón in northern Mexico was “a strategic decision” and that the service will benefit both business and leisure travelers. Travel time will be as much as six hours faster with the nonstop flight than is now available from San Antonio.\n",
    "```\n",
    "\n",
    "Summary: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's bring it all together into a call to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-89EnrBR1fMrnfLyRCG0pD0wRbqbWD at 0x118743c50> JSON: {\n",
       "  \"id\": \"chatcmpl-89EnrBR1fMrnfLyRCG0pD0wRbqbWD\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1697212547,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"San Antonio will be adding another nonstop flight to its list of destinations, this time to Torre\\u00f3n, Mexico. The new route, offered by Viva Aerobus, is seen as a strategic decision that will benefit both business and leisure travelers. Torre\\u00f3n is known for its strong presence in the electronics and automotive industries, making it of potential interest to those involved in San Antonio's growing auto industry. The nonstop flight will also significantly reduce travel time, making it a more convenient option\"\n",
       "      },\n",
       "      \"finish_reason\": \"length\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 248,\n",
       "    \"completion_tokens\": 100,\n",
       "    \"total_tokens\": 348\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down what you're seeing above.\n",
    "\n",
    "You're receiving a JSON object back from the API. The object contains the following:\n",
    "\n",
    "- **model:** The model used to generate the output. As of this writing, it says \"gpt-3.5-turbo-0613\". That's different from what we defined! Here's the deal: OpenAI tweaks their models over time. When you just define \"gpt-3.5-turbo\", you're telling the API to use the latest version of that model. If you want to use a specific version, you can go to the model listing and select which version you want from there.\n",
    "- **choices:** This is the output from the LLM. It's an array of objects. It includes our assistant's message back to us in response to our prompts. You may notice that it finished with an incomplete sentence. This reason for this is because we told the LLM \"hey, only use up to 100 tokens.\" The LLM is incapable of thinking ahead of time that it only has 100 tokens to work with, so it'll just end the sentence when it hits that limit.\n",
    "- **usage:** This is telling us how many tokens we used. Remember, OpenAI charges by the token. Inputs are counted toward the cost.\n",
    "\n",
    "If we only want the output, we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
